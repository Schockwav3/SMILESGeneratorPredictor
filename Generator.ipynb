{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMILES Generator model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate with: NVIDIA GeForce MX150\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/mrbm91/ChemblDatensatzKomplettSmallMolecules.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\SCHOCK~1\\AppData\\Local\\Temp/ipykernel_10248/1834139286.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    776\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    777\u001b[0m \u001b[1;31m# 10. Load and split data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 778\u001b[1;33m \u001b[0msmiles_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    779\u001b[0m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSMILESDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msmiles_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\SCHOCK~1\\AppData\\Local\\Temp/ipykernel_10248/1834139286.py\u001b[0m in \u001b[0;36mload_data\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m    749\u001b[0m     \u001b[0mLoads\u001b[0m \u001b[0mSMILES\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0ma\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    750\u001b[0m     \"\"\"\n\u001b[1;32m--> 751\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    752\u001b[0m         \u001b[0msmiles_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0msmiles_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/mrbm91/ChemblDatensatzKomplettSmallMolecules.txt'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from os import path\n",
    "from typing import List, Tuple, Union\n",
    "import rdkit\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors, AllChem, rdMolDescriptors, rdPartialCharges\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "import csv\n",
    "import random\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import deepsmiles\n",
    "\n",
    "# Suppression of ‘please use MorgenGenerator’ error message \n",
    "from rdkit import rdBase\n",
    "rdBase.DisableLog('rdApp.warning')\n",
    "\n",
    "\n",
    "# User-defined parameters\n",
    "LEARNING_RATE = 0.0010\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 1 #20\n",
    "AUGMENT = True\n",
    "AUGMENT_FACTOR = 5\n",
    "USE_PRETRAINED_MODEL = False\n",
    "LOAD_MODEL_PATH = '-'\n",
    "SAVE_MODEL_PATH = '/home/mrbm91/model_generator_pretrained_2308.pth'\n",
    "FILE_PATH = '/home/mrbm91/chembl_smiles.txt'\n",
    "ACTIVATE_FINE_TUNING = True\n",
    "\n",
    "\n",
    "\n",
    "# 1. Set device for computation (GPU if available, otherwise CPU)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Calculate with: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(f\"Calculate with: {device}\")\n",
    "\n",
    "\n",
    "    \n",
    "# 2. Vocabulary\n",
    "class FixedVocabulary:\n",
    "    def __init__(self):\n",
    "        self.token_to_index = {\n",
    "            'PAD':      0,       # Padding token (for filling unequal batches)\n",
    "            'UNK':      1,       # Undefined token (for unknown elements)\n",
    "            '^':        2,       # Start token\n",
    "            '$':        3,       # End token\n",
    "            '3':        4,\n",
    "            '4':        5,\n",
    "            '5':        6,\n",
    "            '6':        7,\n",
    "            '7':        8,\n",
    "            '8':        9,\n",
    "            '9':        10,\n",
    "            '%10':      11,\n",
    "            '%11':      12,\n",
    "            '%12':      13,\n",
    "            '%13':      14,\n",
    "            '%14':      15,\n",
    "            '%15':      16,\n",
    "            '%16':      17,\n",
    "            '%17':      18,\n",
    "            '%18':      19,\n",
    "            '%19':      20,\n",
    "            '%20':      21,\n",
    "            '%21':      22,\n",
    "            '%22':      23,\n",
    "            '%23':      24,\n",
    "            '%24':      25,\n",
    "            ')':        26,\n",
    "            '=':        27,\n",
    "            '#':        28,\n",
    "            '.':        29,\n",
    "            '-':        30,\n",
    "            '/':        31,\n",
    "            '\\\\':       32, \n",
    "            'n':        33,\n",
    "            'o':        34,\n",
    "            'c':        35,\n",
    "            's':        36,\n",
    "            'N':        37,\n",
    "            'O':        38,\n",
    "            'C':        39,\n",
    "            'S':        40,\n",
    "            'F':        41,\n",
    "            'P':        42,\n",
    "            'I':        43,\n",
    "            'B':        44,\n",
    "            'Br':       45,\n",
    "            'Cl':       46,\n",
    "            '[C@]':     47,\n",
    "            '[C@H]':    48,\n",
    "            '[C@@H]':   49,\n",
    "            '[C@@]':    50,\n",
    "            '[nH]':     51,\n",
    "            '[O-]':     52,\n",
    "            '[N+]':     53,\n",
    "            '[n+]':     54,\n",
    "            '[Na+]':    55,\n",
    "            '[S+]':     56,\n",
    "            '[Br-]':    57,\n",
    "            '[Cl-]':    58,\n",
    "            '[I-]':     59,\n",
    "            '[N-]':     60,\n",
    "            '[Si]':     61,\n",
    "            '[2H]':     62,\n",
    "            '[K+]':     63,\n",
    "            '[Se]':     64,\n",
    "            '[P+]':     65,\n",
    "            '[C-]':     66,\n",
    "            '[se]':     67,\n",
    "            '[Cl+3]:':  68,\n",
    "            '[Li+]:':   69,      \n",
    "        }\n",
    "        self.index_to_token = {v: k for k, v in self.token_to_index.items()}\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.token_to_index)\n",
    "    \n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        This method allows direct access to indices or tokens by treating the vocabulary object as a dictionary.\n",
    "        \"\"\"\n",
    "        if isinstance(key, int):\n",
    "            return self.index_to_token.get(key, '?')  # Returns '?' if the index does not exist\n",
    "        if isinstance(key, str):\n",
    "            return self.token_to_index.get(key, self.token_to_index['UNK'])  # Returns UNK index if the token does not exist\n",
    "\n",
    "\n",
    "    def __contains__(self, item):\n",
    "        \"\"\"\n",
    "        This method checks whether a specific element (token or index) is present in the vocabulary. \n",
    "        It helps to avoid errors by preventing non-existent elements from being processed.\n",
    "        \"\"\"\n",
    "        if isinstance(item, str):\n",
    "            return item in self.token_to_index\n",
    "        elif isinstance(item, int):\n",
    "            return item in self.index_to_token\n",
    "        return False\n",
    "    \n",
    "\n",
    "    def encode(self, tokens):\n",
    "        return [self.token_to_index.get(token, self.token_to_index['UNK']) for token in tokens]\n",
    "    \n",
    "\n",
    "    def decode(self, indices):\n",
    "        return [self.index_to_token.get(index, '?') for index in indices]\n",
    "\n",
    "\n",
    "    def tokens(self):\n",
    "        \"\"\"\n",
    "        A method to return all tokens in the vocabulary. Useful for inspecting the entire vocabulary.\n",
    "        \"\"\"\n",
    "        return list(self.token_to_index.keys())\n",
    "\n",
    "\n",
    "\n",
    "# 3.Tokenizer\n",
    "class DeepSMILESTokenizer:\n",
    "    \"\"\"\n",
    "    Handles the transformation of SMILES into deepSMILES and the\n",
    "    tokenization and untokenization\n",
    "    \"\"\"\n",
    "    def __init__(self, vocabulary):\n",
    "\n",
    "        self.vocabulary = vocabulary\n",
    "        self.converter = deepsmiles.Converter(rings=True, branches=True)\n",
    "\n",
    "        self.REGEXPS = {\n",
    "            \"brackets\": re.compile(r\"(\\[[^\\]]*\\])\"),\n",
    "            \"2_ring_nums\": re.compile(r\"(%\\d{2})\"),\n",
    "            \"brcl\": re.compile(r\"(Br|Cl)\")\n",
    "        }\n",
    "        self.REGEXP_ORDER = [\"brackets\", \"2_ring_nums\", \"brcl\"]\n",
    "\n",
    "\n",
    "    # Encodes SMILES into deepSMILES if a value could not be determined output \"UNK\" token    \n",
    "    def encode(self, smiles: str) -> str:\n",
    "        \"\"\"\n",
    "        Encodes SMILES into deepSMILES.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            deep_smiles = self.converter.encode(smiles)\n",
    "            return deep_smiles\n",
    "        except Exception as e:\n",
    "            print(\"Could not convert SMILES to DeepSMILES:\", e)\n",
    "            return self.vocabulary['UNK']\n",
    "\n",
    "\n",
    "\n",
    "    def tokenize(self, deep_smiles: str, with_begin_and_end: bool = True) -> list:\n",
    "        \"\"\"\n",
    "        Tokenizes a deepSMILES string using the regex and checks if tokens are in vocabulary.\n",
    "        This method ensures that all tokens are either recognized or marked as 'UNK'.\n",
    "        \"\"\"\n",
    "        def split_by(data, regexps):\n",
    "            if not regexps:\n",
    "                return list(data)\n",
    "            regexp = self.REGEXPS[regexps[0]]\n",
    "            splitted = regexp.split(data)\n",
    "            tokens = []\n",
    "            for i, split in enumerate(splitted):\n",
    "                if i % 2 == 0:\n",
    "                    tokens += split_by(split, regexps[1:])\n",
    "                else:\n",
    "                    tokens.append(split)\n",
    "            return [token if token in self.vocabulary else 'UNK' for token in tokens]\n",
    "\n",
    "        tokens = split_by(deep_smiles, self.REGEXP_ORDER)  # Apply regex first\n",
    "        if with_begin_and_end:\n",
    "            tokens = [\"^\"] + tokens + [\"$\"]\n",
    "        return tokens\n",
    "\n",
    "\n",
    "    def untokenize(self, tokens : list) -> str:\n",
    "        \"\"\"\n",
    "        Untokenizes a deepSMILES string, reconstructing it from the token list,\n",
    "        excluding special tokens like '^', '$', and 'UNK'.\n",
    "        \"\"\"\n",
    "        return ''.join(token for token in tokens if token not in ['^', '$', 'UNK'])\n",
    "\n",
    "\n",
    "\n",
    "# 4. Define RNN\n",
    "class RNN(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Implements LSTM including an embedding layer and an output linear layer back to the size of the vocabulary.\n",
    "    \n",
    "     Params:\n",
    "     \n",
    "        voc_size (int)             : Size of the vocabulary (provided by the vocabulary class).\n",
    "        layer_size (int)           : Size of each of the RNN layers.\n",
    "        num_layers (int)           : Number of RNN layers.\n",
    "        embedding_layer_size (int) : Size of the embedding layer.\n",
    "        dropout (float)            : Dropout probabilities.\n",
    "        layer_normalization (bool) : Whether or not to use layer normalization.\n",
    "    \"\"\"\n",
    "\n",
    "    default_params = {\n",
    "        \"layer_size\": 512,\n",
    "        \"num_layers\": 3,\n",
    "        \"embedding_layer_size\": 128,\n",
    "        \"dropout\": 0.2,\n",
    "        \"layer_normalization\": False\n",
    "    }\n",
    "    \n",
    "    def __init__(self, vocabulary, **kwargs):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        # Retrieve vocabulary size directly from the passed vocabulary\n",
    "        voc_size = len(vocabulary)  # Size of the vocabulary\n",
    "\n",
    "        params = {**self.default_params, **kwargs}\n",
    "\n",
    "        self._layer_size                = params['layer_size']\n",
    "        self._embedding_layer_size      = params['embedding_layer_size']\n",
    "        self._num_layers                = params['num_layers']\n",
    "        self._dropout                   = params['dropout']\n",
    "        self._layer_normalization       = params['layer_normalization']\n",
    "\n",
    "\n",
    "        # Initialize the embedding layer with a specific padding_idx\n",
    "        self.embedding = nn.Embedding(num_embeddings=voc_size,\n",
    "                                      embedding_dim=self._embedding_layer_size,\n",
    "                                      padding_idx=vocabulary['PAD']  # Use 'PAD' as the padding index\n",
    "        )\n",
    "        \n",
    "        # LSTM RNN-Setup\n",
    "        self.rnn = nn.LSTM(input_size=self._embedding_layer_size,\n",
    "                           hidden_size=self._layer_size,\n",
    "                           num_layers=self._num_layers,\n",
    "                           dropout=self._dropout if self._num_layers > 1 else 0,\n",
    "                           batch_first=True\n",
    "        )\n",
    "\n",
    "        # Linear output layer mapping RNN output back to the vocabulary size\n",
    "        self.linear = nn.Linear(self._layer_size, voc_size)\n",
    "\n",
    "\n",
    "    def forward(self, input_vector, hidden_state=None):\n",
    "        \"\"\"\n",
    "        Performs a forward pass on the model.\n",
    "        Params:\n",
    "        ------\n",
    "            input_vector (torch.Tensor)         : Input tensor (batch_size, seq_size).\n",
    "            hidden_state (torch.Tensor or None) : Hidden state tensor.\n",
    "        \"\"\"\n",
    "        batch_size, seq_size = input_vector.size()\n",
    "\n",
    "        if hidden_state is None:\n",
    "            device = input_vector.device\n",
    "            hidden_state = (torch.zeros(self._num_layers, batch_size, self._layer_size, device=device),\n",
    "                            torch.zeros(self._num_layers, batch_size, self._layer_size, device=device))\n",
    "\n",
    "        embedded_data = self.embedding(input_vector)\n",
    "        output_vector, hidden_state_out = self.rnn(embedded_data, hidden_state)\n",
    "\n",
    "        if self._layer_normalization:\n",
    "            output_vector = torch.nn.functional.layer_norm(output_vector, output_vector.size()[1:])\n",
    "\n",
    "        output_data = self.linear(output_vector)  # Ensure the output is three-dimensional\n",
    "        return output_data, hidden_state_out\n",
    "    \n",
    "\n",
    "    def get_params(self):\n",
    "        \"\"\"\n",
    "        Returns the configuration parameters of the model.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"layer_size\": self._layer_size,\n",
    "            \"num_layers\": self._num_layers,\n",
    "            \"embedding_layer_size\": self._embedding_layer_size,\n",
    "            \"dropout\": self._dropout,\n",
    "            \"layer_normalization\": self._layer_normalization\n",
    "        }\n",
    "\n",
    "\n",
    "    \n",
    "# 5. Define LSTM\n",
    "class SmilesLSTM(nn.Module):\n",
    "    def __init__(self, vocabulary, tokenizer, max_sequence_length=256, network_params=None):\n",
    "        super(SmilesLSTM, self).__init__()  # Important: Initialize the base class\n",
    "        \n",
    "        self.vocabulary = vocabulary\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        \n",
    "        self.device = device  # CPU / GPU assignment\n",
    "\n",
    "        # Use RNN.default_params if no specific network parameters are provided\n",
    "        if network_params is None:\n",
    "            network_params = {\"voc_size\": len(vocabulary), **RNN.default_params}\n",
    "\n",
    "        self.network = RNN(vocabulary, **network_params)\n",
    "        self.network.to(self.device)\n",
    "\n",
    "        self._nll_loss = nn.NLLLoss(reduction=\"none\")\n",
    "    \n",
    "\n",
    "    def parameters(self):\n",
    "        # This method will allow the model to collect all the parameters of its submodules\n",
    "        return self.network.parameters()\n",
    "\n",
    "    \n",
    "    def forward(self, input):\n",
    "        # Delegate to the RNN module\n",
    "        output, hidden_state = self.network(input)\n",
    "        return output, hidden_state  # Return two values\n",
    "\n",
    "\n",
    "    def likelihood_smiles(self, smiles: list) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Computes the negative log likelihood of generating each SMILES in the input list.\n",
    "        \"\"\"\n",
    "        tokens = [self.tokenizer.tokenize(smile) for smile in smiles]\n",
    "        encoded = [self.vocabulary.encode(token) for token in tokens]\n",
    "        sequences = [torch.tensor(encode, dtype=torch.long, device=self.device) for encode in encoded]\n",
    "        padded_sequences = self.collate_fn(sequences)\n",
    "        return self.likelihood(padded_sequences)\n",
    "\n",
    "\n",
    "    def likelihood(self, sequences: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Retrieves the likelihood of a given sequence. Used in training.\n",
    "        \"\"\"\n",
    "        logits = self.forward(sequences)\n",
    "        log_probs = torch.log_softmax(logits, dim=-1)\n",
    "        return self._nll_loss(log_probs.transpose(1, 2), sequences[:, 1:]).sum(dim=1)\n",
    "    \n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        \"\"\"\n",
    "        Function to collate a list of tensor-encoded sequences into a batch.\n",
    "        Padding-Wert angepasst auf 'PAD' und 0 in der Vocabulary definition\n",
    "        \"\"\"\n",
    "        # `batch` is a list of encoded SMILES sequences that have already been converted into tensors.\n",
    "        # The pad_sequence function takes a list of tensors and pads them to the longest sequence in the batch.\n",
    "        inputs = [item[0] for item in batch if item[0].nelement() != 0]\n",
    "        targets = [item[1] for item in batch if item[1].nelement() != 0]\n",
    "\n",
    "        if not inputs or not targets:\n",
    "            raise ValueError(\"Received an empty batch. Check dataset.\")\n",
    "\n",
    "        inputs_padded = pad_sequence(inputs, batch_first=True, padding_value=vocabulary['PAD'])\n",
    "        targets_padded = pad_sequence(targets, batch_first=True, padding_value=vocabulary['PAD'])\n",
    "    \n",
    "        return inputs_padded, targets_padded\n",
    "\n",
    "\n",
    "    def generate_deepsmiles(self, num_samples: int, max_length: int) -> List[str]:\n",
    "        \"\"\"\n",
    "        This method generates a certain number of DeepSMILES sequences \n",
    "        with a given maximum length by repeatedly calling the generate_deepsmiles_from_start method.\n",
    "    \n",
    "        Args:\n",
    "            num_samples (int): The number of DeepSMILES sequences to generate.\n",
    "            max_length (int): The maximum length of each generated DeepSMILES sequence.\n",
    "        \"\"\"\n",
    "        generated_deepsmiles = []\n",
    "        for _ in range(num_samples):\n",
    "            deep_smiles = self.generate_deepsmiles_from_start(max_length)\n",
    "            generated_deepsmiles.append(deep_smiles)\n",
    "        return generated_deepsmiles\n",
    "\n",
    "\n",
    "    def generate_deepsmiles_from_start(self, max_length=10) -> str:\n",
    "        \"\"\"\n",
    "        This method generates a single DeepSMILES sequence, \n",
    "        by starting at the start token and gradually predicting tokens from the model, \n",
    "        until either the maximum length is reached or the end token is generated.\n",
    "        \"\"\"\n",
    "        self.eval() # Set model to evaluation mode\n",
    "        start_token_id = torch.tensor([self.vocabulary['^']], dtype=torch.long).to(self.device)\n",
    "        input_seq = start_token_id.unsqueeze(0) # Add a batch dimension\n",
    "        generated_tokens = []\n",
    "        for _ in range(max_length):\n",
    "            logits, _ = self.network(input_seq)\n",
    "            probabilities = torch.softmax(logits[:, -1, :], dim=-1)\n",
    "            next_token_id = torch.multinomial(probabilities, 1)\n",
    "            next_token = self.vocabulary.decode([next_token_id.item()])[0]\n",
    "\n",
    "            if next_token == '$':   # Check for end token\n",
    "                break\n",
    "            generated_tokens.append(next_token)\n",
    "\n",
    "            # Update the input sequence with the newly predicted token\n",
    "            # Ensure that next_token_id has the same form as input_seq\n",
    "            next_token_id = next_token_id.view(1, 1)\n",
    "            input_seq = torch.cat([input_seq, next_token_id], dim=1)\n",
    "\n",
    "        return ''.join(generated_tokens)    # Combine all generated tokens into a DeepSMILES sequence\n",
    "    \n",
    "\n",
    "    def convert_deepsmiles_to_smiles(self, deep_smiles_list: List[str]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Converts a generated list of DeepSMILES sequences back to SMILES sequences.\n",
    "        \"\"\"\n",
    "        converter = deepsmiles.Converter(rings=True, branches=True)\n",
    "        smiles_list = []\n",
    "        for deep_smiles in deep_smiles_list:\n",
    "            try:\n",
    "                smiles = converter.decode(deep_smiles)\n",
    "                smiles_list.append(smiles)\n",
    "            except deepsmiles.DecodeError as e:\n",
    "                print(\"Decode Error: Could not convert DeepSMILES to SMILES:\", e)\n",
    "                smiles_list.append(None)\n",
    "        return smiles_list\n",
    "\n",
    "\n",
    "    def load_pretrained_model(self, model_path):\n",
    "        \"\"\"\n",
    "        Loads a pretrained model for Transfer Learing if the variable \"use_pretrained_model\" is True\n",
    "        \"\"\"\n",
    "        self.load_state_dict(torch.load(model_path, map_location=self.device))\n",
    "        print(f\"\\n Loaded pretrained model from {model_path} \\n\")\n",
    "\n",
    "\n",
    "    def save_model(self, model_path):\n",
    "        \"\"\"\n",
    "        Saves the trained model\n",
    "        \"\"\"\n",
    "        torch.save(self.state_dict(), model_path)\n",
    "        print(f\"\\n Model saved to {model_path}\")\n",
    "\n",
    "\n",
    "    def get_params(self):\n",
    "        \"\"\"\n",
    "        Returns the configuration parameters of the model.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            **self.network.get_params(),\n",
    "            \"max_sequence_length\": self.max_sequence_length,\n",
    "            \"vocabulary_size\": len(self.vocabulary),\n",
    "            \"device\": str(self.device)\n",
    "        }\n",
    "    \n",
    "\n",
    "    \n",
    "# 6. Define Trainer\n",
    "class SmilesTrainer:\n",
    "    \"\"\"\n",
    "    Trains a deepSMILES-based generative model using the input SMILES.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, train_dataloader, valid_dataloader, test_dataloader, \n",
    "                 epochs=EPOCHS, learning_rate=LEARNING_RATE, batch_size=BATCH_SIZE, use_pretrained_model=USE_PRETRAINED_MODEL, load_model_path=LOAD_MODEL_PATH, save_model_path=SAVE_MODEL_PATH):\n",
    "        \"\"\"\n",
    "        Initialize the SmilesTrainer with required components and hyperparameters.\n",
    "        Args:\n",
    "            model: The model to be trained.\n",
    "            train_dataloader: DataLoader for training data.\n",
    "            valid_dataloader: DataLoader for validation data.\n",
    "            test_dataloader: DataLoader for testing data.\n",
    "            epochs (int): Number of epochs to train for.\n",
    "            learning_rate (float): Learning rate for the optimizer.\n",
    "            batch_size (int): Batch size for training.\n",
    "            use_pretrained_model (boolean): Decides if a model should train from scratch or with transfer learning.\n",
    "            load_model_path (string): The path to the pretrained model to load.\n",
    "            save_model_path (string): The path to save the trained model.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.model = model\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.valid_dataloader = valid_dataloader\n",
    "        self.test_dataloader = test_dataloader\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        self.train_losses = []\n",
    "        self.valid_losses = []\n",
    "        self.test_losses = []\n",
    "        self.train_accuracy = []\n",
    "        self.valid_accuracy = []\n",
    "        self.test_accuracy = []\n",
    "    \n",
    "        \n",
    "        self.device = device\n",
    "        self.save_model_path = save_model_path\n",
    "\n",
    "        # Set model to the previously learned model if using transfer learning\n",
    "        if use_pretrained_model and load_model_path:\n",
    "            self.model.load_pretrained_model(load_model_path)\n",
    "\n",
    "\n",
    "    def calculate_loss(self, outputs, targets):\n",
    "        # Assumption: outputs are [batch_size, seq_length, num_classes]\n",
    "        # Assumption: targets are [batch_size, seq_length]\n",
    "        \"\"\"\n",
    "        Calculate the negative log likelihood loss.\n",
    "        \"\"\"\n",
    "        log_probs = F.log_softmax(outputs, dim=-1)\n",
    "\n",
    "        # Reshape for loss calculation\n",
    "        log_probs = log_probs.view(-1, log_probs.size(-1))  # [batch_size * seq_length, num_classes]\n",
    "        targets = targets.view(-1)  # [batch_size * seq_length]\n",
    "\n",
    "        # Check if the sizes match\n",
    "        if log_probs.size(0) != targets.size(0):\n",
    "            raise ValueError(f\"Size of the log_probs {log_probs.size(0)} does not match the size of the targets {targets.size(0)}\")\n",
    "\n",
    "        # Calculate the NLLLoss\n",
    "        loss = nn.NLLLoss()(log_probs, targets)\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def calculate_accuracy(self, outputs, targets):\n",
    "        \"\"\"\n",
    "        Calculate the accuracy of the predictions.\n",
    "        \"\"\"\n",
    "        predictions = outputs.argmax(dim=-1)\n",
    "        correct = (predictions == targets).float()\n",
    "        accuracy = correct.mean()\n",
    "        return accuracy.item()\n",
    "\n",
    "\n",
    "    def train_epoch(self):\n",
    "        \"\"\"\n",
    "        Train the model for one epoch.\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        total_accuracy = 0\n",
    "        num_batches = 0\n",
    "\n",
    "        for inputs, targets in self.train_dataloader:\n",
    "            if inputs.nelement() == 0:\n",
    "                continue  # Skip empty batches\n",
    "\n",
    "            inputs, targets = inputs.to(self.device), targets.to(self.device)  # Move data to the appropriate device\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs, _ = self.model(inputs)  # Only use the first value from the tuple\n",
    "            loss = self.calculate_loss(outputs, targets)\n",
    "            accuracy = self.calculate_accuracy(outputs, targets)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_accuracy += accuracy\n",
    "            num_batches += 1\n",
    "    \n",
    "        average_loss = total_loss / num_batches if num_batches > 0 else float('inf')\n",
    "        average_accuracy = total_accuracy / num_batches if num_batches > 0 else 0\n",
    "        self.train_losses.append(average_loss)\n",
    "        self.train_accuracy.append(average_accuracy)\n",
    "        return average_loss, average_accuracy\n",
    "\n",
    "\n",
    "    def validate(self):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        total_accuracy = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in self.valid_dataloader:\n",
    "                inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "                outputs, _ = self.model(inputs)  # Only use the first value from the tuple\n",
    "                loss = self.calculate_loss(outputs, targets)\n",
    "                accuracy = self.calculate_accuracy(outputs, targets)\n",
    "                total_loss += loss.item()\n",
    "                total_accuracy += accuracy\n",
    "\n",
    "        average_loss = total_loss / len(self.valid_dataloader)\n",
    "        average_accuracy = total_accuracy / len(self.valid_dataloader)\n",
    "        self.valid_losses.append(average_loss)\n",
    "        self.valid_accuracy.append(average_accuracy)\n",
    "        return average_loss, average_accuracy\n",
    "\n",
    "    def test(self):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        total_accuracy = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in self.test_dataloader:\n",
    "                inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "                outputs, _ = self.model(inputs)  # Only use the first value from the tuple\n",
    "                loss = self.calculate_loss(outputs, targets)\n",
    "                accuracy = self.calculate_accuracy(outputs, targets)\n",
    "                total_loss += loss.item()\n",
    "                total_accuracy += accuracy\n",
    "\n",
    "        average_loss = total_loss / len(self.test_dataloader)\n",
    "        average_accuracy = total_accuracy / len(self.test_dataloader)\n",
    "        self.test_losses.append(average_loss)\n",
    "        self.test_accuracy.append(average_accuracy)\n",
    "        return average_loss, average_accuracy\n",
    "\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Executes the training process over the specified number of epochs.\n",
    "        \"\"\"\n",
    "        for epoch in range(self.epochs):\n",
    "            train_loss, train_acc = self.train_epoch()\n",
    "            valid_loss, valid_acc = self.validate()\n",
    "            test_loss, test_acc = self.test()\n",
    "            print(f\"Epoch {epoch+1}/{self.epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Validation Loss: {valid_loss:.4f}, Valid Acc: {valid_acc:.4f}, Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}\")\n",
    "        \n",
    "        if self.save_model_path:\n",
    "            self.model.save_model(self.save_model_path)\n",
    "        \n",
    "        self.plot_losses()\n",
    "        self.plot_accuracy()\n",
    "\n",
    "\n",
    "    def plot_losses(self):\n",
    "        \"\"\"\n",
    "        Plot training, validation, and test loss progress.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(self.train_losses, label='Training loss')\n",
    "        plt.plot(self.valid_losses, label='Validation loss')\n",
    "        plt.plot(self.test_losses, label='Test loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training Loss Progress')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    def plot_accuracy(self):\n",
    "        \"\"\"\n",
    "        Plot training, validation, and test accuracy progress.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(self.train_accuracy, label='Training accuracy')\n",
    "        plt.plot(self.valid_accuracy, label='Validation accuracy')\n",
    "        plt.plot(self.test_accuracy, label='Test accuracy')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title('Training Accuracy Progress')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# 7. Define Dataset\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "class SMILESDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Parameter to adjust augmentation.\n",
    "    \n",
    "    Args:\n",
    "        augment: If the augmentation should be activated.\n",
    "        augment_factor: Number of different permutations per SMILE which should be generated.\n",
    "    \"\"\"\n",
    "    def __init__(self, smiles_list, tokenizer, vocabulary, augment=AUGMENT, augment_factor=AUGMENT_FACTOR):\n",
    "        super(SMILESDataset, self).__init__()\n",
    "        self.inputs = []\n",
    "        self.targets = []\n",
    "        self.augment = augment\n",
    "        self.augment_factor = augment_factor\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocabulary = vocabulary\n",
    "\n",
    "        for index, smile in enumerate(smiles_list):\n",
    "            try:\n",
    "                if index % 10000 == 0:  # Log every 10000th step\n",
    "                    logging.info(f\"Processing {index}/{len(smiles_list)} SMILES:\")\n",
    "\n",
    "\n",
    "                # Perform augmentation on SMILES if activated\n",
    "                # Generate different valid SMILES representations\n",
    "                # Otherwise, augmented_smiles only contains the original SMILES string\n",
    "                augmented_smiles = self.randomize_smiles(smile, self.augment_factor) if self.augment else [smile]\n",
    "\n",
    "\n",
    "                for sm in augmented_smiles:\n",
    "                    # Convert SMILES to deepSMILES\n",
    "                    deep_smiles = tokenizer.encode(sm)\n",
    "                    tokenized = tokenizer.tokenize(deep_smiles)\n",
    "                    encoded = vocabulary.encode(tokenized)\n",
    "\n",
    "                    if not encoded:\n",
    "                        logging.warning(f\"Empty sequence found for SMILES '{sm}'. Skipping.\")\n",
    "                        continue  \n",
    "\n",
    "                # Generate full input and target sequences\n",
    "                self.inputs.append(torch.tensor(encoded[:-1], dtype=torch.long))  # All except the last token\n",
    "                self.targets.append(torch.tensor(encoded[1:], dtype=torch.long))  # All except the first token\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing SMILE '{smile}': {e}\")\n",
    "                continue    # Optionally, move to the next SMILE\n",
    "\n",
    "        if not self.inputs or not self.targets:\n",
    "            logging.warning(\"No valid data was processed. Check tokenization and encoding steps.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_tensor, target_tensor = self.inputs[idx], self.targets[idx]\n",
    "        # print(f\"Get item {idx}: Input size {input_tensor.shape}, Target size {target_tensor.shape}\")\n",
    "        return input_tensor, target_tensor\n",
    "    \n",
    "\n",
    "    # Augmentation randomize using valid SMILES representations\n",
    "    def randomize_smiles(self, smile: str, num_random: int) -> List[str]:\n",
    "        mol = Chem.MolFromSmiles(smile)\n",
    "        if mol is None:\n",
    "            return [smile] * num_random\n",
    "        return [Chem.MolToSmiles(mol, doRandom=True) for _ in range(num_random)]\n",
    "\n",
    "\n",
    "def load_data(file_path):\n",
    "    \"\"\"\n",
    "    Loads SMILES data from a file.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        smiles_list = file.read().splitlines()\n",
    "    return smiles_list\n",
    "\n",
    "\n",
    "def split_data(dataset, train_split=0.7, valid_split=0.15, test_split=0.15):\n",
    "    \"\"\"\n",
    "    Splits dataset into training, validation, and test sets.\n",
    "    \"\"\"\n",
    "    train_size = int(len(dataset) * train_split)\n",
    "    valid_size = int(len(dataset) * valid_split)\n",
    "    test_size = len(dataset) - train_size - valid_size\n",
    "    return random_split(dataset, [train_size, valid_size, test_size])\n",
    "\n",
    "\n",
    "\n",
    "# 8. Path to the SMILES file for learning the generative model\n",
    "file_path = FILE_PATH\n",
    "\n",
    "\n",
    "# 9. Define Vocabulary, Tokenizer, and Model\n",
    "vocabulary = FixedVocabulary()\n",
    "tokenizer = DeepSMILESTokenizer(vocabulary)\n",
    "model = SmilesLSTM(vocabulary, tokenizer)\n",
    "\n",
    "\n",
    "# 10. Load and split data\n",
    "smiles_list = load_data(file_path)\n",
    "dataset = SMILESDataset(smiles_list, tokenizer, vocabulary)\n",
    "\n",
    "\n",
    "\n",
    "# Debugging: SMILES to deepSMILES\n",
    "for index, smile in enumerate(smiles_list[:5]):\n",
    "    try:\n",
    "        deep_smiles = tokenizer.encode(smile)\n",
    "        print(f\"SMILES {index+1}/{len(smiles_list)}: {smile}\")\n",
    "        print(f\"DeepSMILES {index+1}/{len(smiles_list)}: {deep_smiles} \\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing SMILE '{smile}': {e} \\n\")\n",
    "\n",
    "\n",
    "# Debugging\n",
    "sample_smiles = smiles_list[0]  # Take the first SMILES from the list\n",
    "print(\"Testing SMILES:\", sample_smiles)\n",
    "deep_smiles = tokenizer.encode(sample_smiles)\n",
    "tokenized = tokenizer.tokenize(deep_smiles)\n",
    "encoded = vocabulary.encode(tokenized)\n",
    "print(f\"DeepSMILES: {deep_smiles}\") # Take the first converted deepSMILES\n",
    "print(f\"Encoded sequence (DeepSMILE): {encoded} \\n\")  # Show the encoded deepSMILES sequences\n",
    "\n",
    "\n",
    "# Debugging\n",
    "print(\"Number of sequences in dataset:\", len(dataset))\n",
    "for i, (input_seq, target_seq) in enumerate(dataset):\n",
    "    if i < 10:  # print first 10 sequences\n",
    "        print(f\"Input {i}: {input_seq.shape}, Target {i}: {target_seq.shape}\")\n",
    "    if input_seq.nelement() == 0 or target_seq.nelement() == 0:\n",
    "        print(f\"Empty sequence found at index {i}\")\n",
    "\n",
    "\n",
    "\n",
    "# Split data\n",
    "train_dataset, valid_dataset, test_dataset = split_data(dataset)\n",
    "\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=SmilesLSTM.collate_fn)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=SmilesLSTM.collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=SmilesLSTM.collate_fn)\n",
    "\n",
    "\n",
    "\n",
    "# Debugging: Padding\n",
    "for inputs, targets in train_loader:\n",
    "    print(f\"\\n Batch Inputs Shape: {inputs.shape}\")  # All batches should have the same second dimension\n",
    "    print(f\"Batch Targets Shape: {targets.shape}\")\n",
    "    print(\"\\n Sample Input Batch:\", inputs[0])  # Show a sample input batch\n",
    "    print(\"Sample Target Batch: \\n\", targets[0])  # Show a sample target batch\n",
    "    break  # Only print the first batch\n",
    "\n",
    "\n",
    "\n",
    "# 11. Initialize und train the model\n",
    "trainer = SmilesTrainer(model, train_loader, valid_loader, test_loader)\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "# 12. Define the variable to control fine-tuning\n",
    "activate_fine_tuning = ACTIVATE_FINE_TUNING  # Set to True to perform fine-tuning (Phase 2), False to skip\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#------------------(fine-tuning Phase 2) Reinforcement Learning mit Predictor MLP--------------------------\n",
    "\n",
    "\"\"\"\n",
    "    Function:\n",
    "    1. Sampling: The model generates a series of sequences (DeepSMILES).\n",
    "    2. Rewarding: Each sequence is given a reward based on its quality.\n",
    "    3. Log-Probs Calculation: For each sequence, the log probability of its generation is calculated.\n",
    "    4. Loss Calculation: The loss is calculated as the negative product of the log probabilities and the rewards.\n",
    "    5. Backpropagation: The calculated loss is used for backpropagation to compute the gradients and update\n",
    "    the model's weights.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# User-defined parameters (Phase 2)\n",
    "TRAINED_LSTM_PATH = '/home/mrbm91/model_generator_pretrained_1908.pth'\n",
    "TRAINED_MLP_PATH = '/home/mrbm91/model_predictor_1908_final3.pth'\n",
    "FINE_TUNE_EPOCHS = 150\n",
    "FINE_TUNE_LEARNING_RATE = 0.00002\n",
    "FINE_TUNE_BATCH_SIZE = 32\n",
    "FINE_TUNE_SAVE_MODEL_PATH = '/home/mrbm91/finetuned_model2308.pth'\n",
    "NUM_GENERATED_SMILES = 400\n",
    "REWARD_SCALE = 1\n",
    "\n",
    "\n",
    "\n",
    "# 1. Path to your trained LSTM and MLP model\n",
    "trained_lstm_path = TRAINED_LSTM_PATH\n",
    "trained_mlp_path = TRAINED_MLP_PATH\n",
    "\n",
    "\n",
    "# 2. Define MLP Validator\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 512)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.fc3 = nn.Linear(256, 64)\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "        self.fc4 = nn.Linear(64, 32)\n",
    "        self.bn4 = nn.BatchNorm1d(32)\n",
    "        self.fc5 = nn.Linear(32, 8)\n",
    "        self.bn5 = nn.BatchNorm1d(8)\n",
    "        self.fc6 = nn.Linear(8, 2)\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.fc4(x))\n",
    "        x = torch.relu(self.fc5(x))\n",
    "        x = self.fc6(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# 3. Function to calculate descriptors\n",
    "def calculate_descriptors(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return None\n",
    "\n",
    "    descriptors = {\n",
    "        \"MorganFingerprint\":    AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=2048),\n",
    "        \"AlogP\":                Descriptors.MolLogP(mol),\n",
    "        \"PolarSurfaceArea\":     Descriptors.TPSA(mol),\n",
    "        \"HBA\":                  Descriptors.NumHAcceptors(mol),\n",
    "        \"HBD\":                  Descriptors.NumHDonors(mol),\n",
    "        \"Chi0\":                 Descriptors.Chi0(mol),\n",
    "        \"Kappa1\":               Descriptors.Kappa1(mol),\n",
    "        \"TPSA\":                 Descriptors.TPSA(mol),\n",
    "        \"MolLogP\":              Descriptors.MolLogP(mol),\n",
    "        \"PEOE_VSA1\":            Descriptors.PEOE_VSA1(mol),\n",
    "        \"PEOE_VSA2\":            Descriptors.PEOE_VSA2(mol),\n",
    "        \"PEOE_VSA3\":            Descriptors.PEOE_VSA3(mol),\n",
    "        \"PEOE_VSA4\":            Descriptors.PEOE_VSA4(mol),\n",
    "        \"PEOE_VSA5\":            Descriptors.PEOE_VSA5(mol),\n",
    "        \"PEOE_VSA6\":            Descriptors.PEOE_VSA6(mol),\n",
    "        \"PEOE_VSA7\":            Descriptors.PEOE_VSA7(mol),\n",
    "        \"PEOE_VSA8\":            Descriptors.PEOE_VSA8(mol),\n",
    "        \"PEOE_VSA9\":            Descriptors.PEOE_VSA9(mol),\n",
    "        \"PEOE_VSA10\":           Descriptors.PEOE_VSA10(mol),\n",
    "        \"PEOE_VSA11\":           Descriptors.PEOE_VSA11(mol),\n",
    "        \"PEOE_VSA12\":           Descriptors.PEOE_VSA12(mol),\n",
    "        \"PEOE_VSA13\":           Descriptors.PEOE_VSA13(mol),\n",
    "        \"PEOE_VSA14\":           Descriptors.PEOE_VSA14(mol),\n",
    "        \"MolecularWeight\":      Descriptors.MolWt(mol),\n",
    "        \"NumRotatableBonds\":    Descriptors.NumRotatableBonds(mol),\n",
    "        \"NumAromaticRings\":     Descriptors.NumAromaticRings(mol),\n",
    "        #\"MaxPartialCharge\":     max_partial_charge,\n",
    "        #\"MinPartialCharge\":     min_partial_charge,\n",
    "        \"FractionCSP3\":         Descriptors.FractionCSP3(mol),\n",
    "        #\"RadiusOfGyration\":     rdMolDescriptors.CalcRadiusOfGyration(mol),\n",
    "        #\"Polarizability\":       rdMolDescriptors.CalcExactMolWt(mol),\n",
    "        #\"MolVolume\":            Descriptors.MolWt(mol),\n",
    "        #\"MolWt\":                Descriptors.MolWt(mol),\n",
    "        \"HeavyAtomCount\":       Descriptors.HeavyAtomCount(mol),\n",
    "        \"NHOHCount\":            Descriptors.NHOHCount(mol),\n",
    "        \"NOCount\":              Descriptors.NOCount(mol),\n",
    "        \"NumHeteroatoms\":       Descriptors.NumHeteroatoms(mol),\n",
    "        \"NumRadicalElectrons\":  Descriptors.NumRadicalElectrons(mol),\n",
    "        \"NumValenceElectrons\":  Descriptors.NumValenceElectrons(mol),\n",
    "        \"RingCount\":            Descriptors.RingCount(mol),\n",
    "        \"BalabanJ\":             Descriptors.BalabanJ(mol),\n",
    "        \"BertzCT\":              Descriptors.BertzCT(mol),\n",
    "        \"Chi1\":                 Descriptors.Chi1(mol),\n",
    "        \"Chi0n\":                Descriptors.Chi0n(mol),\n",
    "        \"Chi0v\":                Descriptors.Chi0v(mol),\n",
    "        \"Chi1n\":                Descriptors.Chi1n(mol),\n",
    "        \"Chi1v\":                Descriptors.Chi1v(mol),\n",
    "        \"Kappa2\":               Descriptors.Kappa2(mol),\n",
    "        \"Kappa3\":               Descriptors.Kappa3(mol),\n",
    "        \"HallKierAlpha\":        Descriptors.HallKierAlpha(mol),\n",
    "    }\n",
    "    return descriptors\n",
    "\n",
    "\n",
    "# Function to load the MLP model\n",
    "def load_mlp_model(model_path, input_size):\n",
    "    \"\"\"\n",
    "    Load the MLP model.\n",
    "    \"\"\"\n",
    "    model = MLP(input_size)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Check if a SMILES string is valid\n",
    "def is_valid_smiles(smiles):\n",
    "    \"\"\"\n",
    "    Validate a SMILES string.\n",
    "    \"\"\"\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    return mol is not None\n",
    "\n",
    "\n",
    "# Evaluate SMILES using MLP\n",
    "def evaluate_smiles(smiles, mlp_model, tokenizer, vocabulary):\n",
    "    \"\"\"\n",
    "    Evaluate a SMILES string using the MLP model.\n",
    "    \"\"\"\n",
    "    descriptors = calculate_descriptors(smiles)\n",
    "    if descriptors is None:\n",
    "        return 0  # Invalid SMILES, return 0 score\n",
    "    fingerprint = list(descriptors.pop(\"MorganFingerprint\"))\n",
    "    descriptors_list = list(descriptors.values()) + fingerprint\n",
    "    descriptors_tensor = torch.tensor(descriptors_list, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = mlp_model(descriptors_tensor)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "    return predicted.item()\n",
    "\n",
    "\n",
    "# 4. Define the vocabulary, tokanizer, LSTM-Structure\n",
    "vocabulary = FixedVocabulary()\n",
    "tokenizer = DeepSMILESTokenizer(vocabulary)\n",
    "smiles_lstm_model = SmilesLSTM(vocabulary, tokenizer)\n",
    "smiles_lstm_model.load_pretrained_model(trained_lstm_path)  # Load the Pre-trained model\n",
    "smiles_lstm_model.to(device)\n",
    "\n",
    "input_size = 2048 + 43  # Adjust based on the actual number of features (MorganFingerprints + Descriptors)\n",
    "mlp_model = load_mlp_model(trained_mlp_path, input_size)\n",
    "\n",
    "\n",
    "# 5. New train method for (Phase 2)\n",
    "class SmilesTrainerPhase2:\n",
    "    \"\"\"\n",
    "        Initialize the SmilesTrainerPhase2 with required components and hyperparameters.\n",
    "\n",
    "        Args:\n",
    "            model: The model to be trained.\n",
    "            validator_model: The MLP model for validation.\n",
    "            num_generated_smiles (int): Number of SMILES to generate per epoch.\n",
    "            epochs (int): Number of epochs to train for.\n",
    "            learning_rate (float): Learning rate for the optimizer.\n",
    "            batch_size (int): Batch size for training.\n",
    "            save_model_path (str): The path to save the trained model.\n",
    "            reward_scale (float): Scale factor for rewards.\n",
    "        \"\"\"\n",
    "    def __init__(self, model, validator_model, num_generated_smiles=NUM_GENERATED_SMILES, epochs=FINE_TUNE_EPOCHS, learning_rate=FINE_TUNE_LEARNING_RATE, batch_size=FINE_TUNE_BATCH_SIZE, save_model_path=FINE_TUNE_SAVE_MODEL_PATH, reward_scale=REWARD_SCALE):\n",
    "        self.model = model\n",
    "        self.validator_model = validator_model\n",
    "        self.num_generated_smiles = num_generated_smiles\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.save_model_path = save_model_path\n",
    "        self.reward_scale = reward_scale\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        self.train_losses = []\n",
    "        self.train_accuracy = []\n",
    "        self.total_accuracy = []\n",
    "        self.generated_smiles_counts = []\n",
    "        self.axl_classified_counts = []\n",
    "\n",
    "\n",
    "    def calculate_accuracy(self, axl_classified_count, num_generated_smiles):\n",
    "        \"\"\"\n",
    "        Calculate the accuracy of AXL classified SMILES in relation to valid SMILES.\n",
    "        \"\"\"\n",
    "        return axl_classified_count / num_generated_smiles if num_generated_smiles > 0 else 0\n",
    "    \n",
    "\n",
    "    def calculate_total_accuracy(self, axl_classified_count, generated_smiles_count):\n",
    "        \"\"\"\n",
    "        Calculate the accuracy of AXL classified SMILES in relation to all SMILES including invalid ones.\n",
    "        \"\"\"\n",
    "        return axl_classified_count / generated_smiles_count if generated_smiles_count > 0 else 0\n",
    "\n",
    "\n",
    "    def train_epoch(self):\n",
    "        \"\"\"\n",
    "        Train the model for one epoch.\n",
    "        \"\"\"\n",
    "        self.model.train()  # Ensure the model is in training mode\n",
    "        total_loss = 0\n",
    "        total_accuracy = 0\n",
    "        num_batches = 0\n",
    "\n",
    "            \n",
    "        # Initiate lists and counts for calculations and outputs\n",
    "        total_reward = 0\n",
    "        invalid_deepsmiles_count = 0\n",
    "        valid_smiles_count = 0\n",
    "        invalid_smiles_count = 0\n",
    "            \n",
    "        axl_classified_count = 0\n",
    "        non_axl_classified_count = 0\n",
    "            \n",
    "        generated_deepsmiles_list = []\n",
    "        invalid_deepsmiles_list = []\n",
    "        \n",
    "        generated_smiles_list = []\n",
    "        valid_smiles_list = []\n",
    "        invalid_smiles_list = []\n",
    "            \n",
    "        axl_classified_smiles_list = []\n",
    "        non_axl_classified_smiles_list = []\n",
    "        \n",
    "        rewards = []\n",
    "        log_probs = []\n",
    "            \n",
    "\n",
    "        while valid_smiles_count < self.num_generated_smiles:\n",
    "            generated_deepsmiles = self.model.generate_deepsmiles(num_samples=1, max_length=100)[0]\n",
    "            generated_deepsmiles_list.append(generated_deepsmiles)\n",
    "    \n",
    "            # If DeepSmile is faulty during generation\n",
    "            try:\n",
    "                generated_smiles = self.model.convert_deepsmiles_to_smiles([generated_deepsmiles])[0]\n",
    "                if generated_smiles is None:\n",
    "                    raise ValueError(\"Conversion to SMILES resulted in None\")\n",
    "            except (deepsmiles.DecodeError, IndexError, ValueError):\n",
    "                invalid_deepsmiles_list.append(generated_deepsmiles)\n",
    "                invalid_deepsmiles_count += 1\n",
    "                continue  # Skip this faulty DeepSMILES string\n",
    "        \n",
    "            generated_smiles_list.append(generated_smiles)\n",
    "\n",
    "            # If DeepSmile is valid\n",
    "            if is_valid_smiles(generated_smiles):\n",
    "                valid_smiles_list.append(generated_smiles)\n",
    "                valid_smiles_count += 1\n",
    "                score = evaluate_smiles(generated_smiles, self.validator_model, self.model.tokenizer, self.model.vocabulary)\n",
    "                \n",
    "                if score == 1:  # If DeepSmile is classified as AXL Kinase Inhibitor\n",
    "                    axl_classified_count += 1\n",
    "                    axl_classified_smiles_list.append(generated_smiles)\n",
    "                    reward = 2.5  # Reward\n",
    "\n",
    "                else:   # If valid but not classified as target\n",
    "                    non_axl_classified_count += 1\n",
    "                    non_axl_classified_smiles_list.append(generated_smiles)\n",
    "                    reward = 0.5  # Small reward for valid but not AXL-classified SMILES\n",
    "                    \n",
    "                total_reward += reward\n",
    "                rewards.append(reward)\n",
    "                \n",
    "                # Calculate the log probability of the action (generated sequence)\n",
    "                tokenized = self.model.tokenizer.tokenize(generated_deepsmiles)\n",
    "                encoded = self.model.vocabulary.encode(tokenized)\n",
    "                sequence = torch.tensor(encoded, dtype=torch.long, device=self.model.device).unsqueeze(0)\n",
    "                self.model.train()  # Ensure the model is in training model\n",
    "                logit, _ = self.model(sequence)\n",
    "                log_prob = F.log_softmax(logit, dim=-1)\n",
    "                log_prob = log_prob.gather(2, sequence[:, 1:].unsqueeze(-1)).squeeze(-1)\n",
    "                log_probs.append(log_prob.sum())\n",
    "            \n",
    "            else:\n",
    "                invalid_smiles_count += 1\n",
    "                invalid_smiles_list.append(generated_smiles)\n",
    "                reward = -7.5  # Penalty for invalid SMILES\n",
    "                total_reward += reward\n",
    "\n",
    "        \n",
    "        # Reduced rewards through normalization and scaling\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32, device=self.model.device)\n",
    "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-5) # Normalization\n",
    "        rewards *= self.reward_scale  # Scaling\n",
    "\n",
    "        \n",
    "        # Calculate the Policy Gradient loss\n",
    "        log_probs = torch.stack(log_probs)\n",
    "        policy_loss = -log_probs * rewards\n",
    "        policy_loss = policy_loss.mean()\n",
    "\n",
    "            \n",
    "        # Backpropagation\n",
    "        self.optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        \n",
    "        total_loss += policy_loss.item()\n",
    "        total_accuracy += self.calculate_accuracy(axl_classified_count, self.num_generated_smiles)\n",
    "        total_acc = self.calculate_total_accuracy(axl_classified_count, len(generated_smiles_list))\n",
    "        self.total_accuracy.append(total_acc)\n",
    "        num_batches += 1\n",
    "\n",
    "        \n",
    "        # Log the generated SMILES and rewards\n",
    "        print(f\"Generated {len(generated_smiles_list)} SMILES to obtain {self.num_generated_smiles} valid SMILES.\")\n",
    "        print(f\"Invalid SMILES count: {invalid_smiles_count}\")\n",
    "            \n",
    "        print(f\"AXL classified count: {axl_classified_count}\")\n",
    "        print(f\"Non-AXL classified count: {non_axl_classified_count}\")\n",
    "            \n",
    "        print(f\"Total reward: {total_reward:.4f}\")\n",
    "        #print(f\"Generated SMILES: {generated_smiles_list}\")\n",
    "        #print(f\"AXL Classified SMILES: {axl_classified_smiles_list}\")\n",
    "        #print(f\"Non-AXL Classified SMILES: {non_axl_classified_smiles_list}\")\n",
    "\n",
    "        \n",
    "        average_loss = total_loss / num_batches if num_batches > 0 else float('inf')\n",
    "        average_accuracy = axl_classified_count / self.num_generated_smiles if valid_smiles_count > 0 else 0\n",
    "        self.train_losses.append(average_loss)\n",
    "        self.train_accuracy.append(average_accuracy)\n",
    "        self.generated_smiles_counts.append(len(generated_smiles_list))\n",
    "        self.axl_classified_counts.append(axl_classified_count)\n",
    "        return average_loss, average_accuracy\n",
    "\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Executes the training process over the specified number of epochs.\n",
    "        \"\"\"\n",
    "        for epoch in range(self.epochs):\n",
    "            train_loss, train_acc = self.train_epoch()\n",
    "            total_acc = self.total_accuracy[-1] if self.total_accuracy else 0  # Current total accuracy\n",
    "            print(f\"Epoch {epoch+1}/{self.epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Total Acc: {total_acc:.4f}\")\n",
    "        \n",
    "        \n",
    "        if self.save_model_path:\n",
    "            self.model.save_model(self.save_model_path)\n",
    "\n",
    "\n",
    "\n",
    "# Functions for plotting\n",
    "def plot_accuracy(train_accuracy, total_accuracy):\n",
    "    \"\"\"\n",
    "    Plot training and total accuracy progress.\n",
    "    \"\"\"\n",
    "    epochs = range(1, len(train_accuracy) + 1)\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.plot(epochs, train_accuracy, label='Training Accuracy')\n",
    "    plt.plot(epochs, total_accuracy, label='Total Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Training and Total Accuracy Progress')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_generated_smiles(generated_smiles_counts, axl_classified_counts):\n",
    "    \"\"\"\n",
    "    Plot generated SMILES and AXL classified SMILES count.\n",
    "    \"\"\"\n",
    "    epochs = range(1, len(generated_smiles_counts) + 1)\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.plot(epochs, generated_smiles_counts, label='Generated SMILES')\n",
    "    plt.plot(epochs, axl_classified_counts, label='AXL Classified SMILES')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Generated SMILES and AXL Classified SMILES Count')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "        \n",
    "# Check if fine-tuning is activated\n",
    "if activate_fine_tuning:\n",
    "    \n",
    "    # Initialize and train the fine-tuning model with reinforcement learning (Phase 2)\n",
    "    trainer_phase2 = SmilesTrainerPhase2(smiles_lstm_model, mlp_model)\n",
    "    trainer_phase2.train()\n",
    "    \n",
    "\n",
    "    # Plot Accuracy /  Generated SMILES and AXL Classified SMILES\n",
    "    plot_accuracy(trainer_phase2.train_accuracy, trainer_phase2.total_accuracy)\n",
    "    plot_generated_smiles(trainer_phase2.generated_smiles_counts, trainer_phase2.axl_classified_counts)\n",
    "    \n",
    "    \n",
    "else:\n",
    "    print(\"Fine-tuning is not activated. Only the LSTM model training was performed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
