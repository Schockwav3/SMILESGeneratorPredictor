{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python Script um aus einer Liste von SMILES herrauszufinden welche Periodensymbole und wie oft vorkommen um ein Vocabular für das Neuronale Netz zu geniereren \n",
    "\n",
    "1.SMILES\n",
    "2.DEEPSMILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Union\n",
    "import rdkit\n",
    "import numpy as np\n",
    "from typing import Union, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    \"\"\"\n",
    "    Stores the tokens and their conversion to vocabulary indexes, along with their frequencies.\n",
    "    \"\"\"\n",
    "    def __init__(self, tokens: Union[Dict[str, int], None] = None, starting_id: int = 0) -> None:\n",
    "        self._tokens = {}\n",
    "        self._frequencies = {}\n",
    "        self._current_id = starting_id\n",
    "\n",
    "        if tokens:\n",
    "            for token, idx in tokens.items():\n",
    "                self._add(token, idx)\n",
    "                self._current_id = max(self._current_id, idx + 1)\n",
    "\n",
    "    def __getitem__(self, token_or_id: Union[str, int]) -> int:\n",
    "        \"\"\"\n",
    "        Allows getting the index of a token or token of an index.\n",
    "        \"\"\"\n",
    "        return self._tokens[token_or_id]\n",
    "\n",
    "    def add(self, token: str) -> int:\n",
    "        \"\"\"\n",
    "        Adds a token to the vocabulary or updates its frequency if it already exists.\n",
    "        \"\"\"\n",
    "        if not isinstance(token, str):\n",
    "            raise TypeError(\"Token is not a string\")\n",
    "        if token in self._tokens:\n",
    "            self._frequencies[token] += 1\n",
    "            return self._tokens[token]\n",
    "        else:\n",
    "            self._add(token, self._current_id)\n",
    "            self._frequencies[token] = 1\n",
    "            self._current_id += 1\n",
    "            return self._current_id - 1\n",
    "\n",
    "    def update(self, tokens: list) -> None:\n",
    "        \"\"\"\n",
    "        Adds multiple tokens to the vocabulary.\n",
    "        \"\"\"\n",
    "        for token in tokens:\n",
    "            self.add(token)\n",
    "\n",
    "    def __delitem__(self, token_or_id: Union[str, int]) -> None:\n",
    "        \"\"\"\n",
    "        Deletes a token or index from the vocabulary.\n",
    "        \"\"\"\n",
    "        other_val = self._tokens[token_or_id]\n",
    "        del self._tokens[other_val]\n",
    "        del self._tokens[token_or_id]\n",
    "        del self._frequencies[other_val if isinstance(token_or_id, int) else token_or_id]\n",
    "\n",
    "    def __contains__(self, token_or_id: Union[str, int]) -> bool:\n",
    "        \"\"\"\n",
    "        Checks if a token or index is in the vocabulary.\n",
    "        \"\"\"\n",
    "        return token_or_id in self._tokens\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Returns the number of unique tokens.\n",
    "        \"\"\"\n",
    "        return len(self._tokens) // 2\n",
    "\n",
    "    def tokens(self) -> list:\n",
    "        \"\"\"\n",
    "        Returns the tokens from the vocabulary sorted by their frequencies in descending order.\n",
    "        \"\"\"\n",
    "        return sorted(self._frequencies, key=self._frequencies.get, reverse=True)\n",
    "\n",
    "    def frequencies(self) -> Dict[str, int]:\n",
    "        \"\"\"\n",
    "        Returns a dictionary of token frequencies.\n",
    "        \"\"\"\n",
    "        return self._frequencies\n",
    "\n",
    "    def _add(self, token: str, idx: int) -> None:\n",
    "        \"\"\"\n",
    "        Adds a token and its index to the internal dictionaries.\n",
    "        \"\"\"\n",
    "        if idx not in self._tokens:\n",
    "            self._tokens[token] = idx\n",
    "            self._tokens[idx] = token\n",
    "            self._frequencies[token] = 0  # Initialize frequency\n",
    "        else:\n",
    "            raise ValueError(\"IDX already present in vocabulary\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SMILESTokenizer:\n",
    "    \"\"\"\n",
    "    Handles the tokenization and untokenization of SMILES.\n",
    "    \"\"\"\n",
    "\n",
    "    REGEXPS = {\n",
    "        \"brackets\": re.compile(r\"(\\[[^\\]]*\\])\"),\n",
    "        \"2_ring_nums\": re.compile(r\"(%\\d{2})\"),\n",
    "        \"brcl\": re.compile(r\"(Br|Cl)\")\n",
    "    }\n",
    "    REGEXP_ORDER = [\"brackets\", \"2_ring_nums\", \"brcl\"]\n",
    "\n",
    "    def tokenize(self, data : str, with_begin_and_end : bool=True) -> list:\n",
    "        \"\"\"\n",
    "        Tokenizes a SMILES string.\n",
    "        \"\"\"\n",
    "        def split_by(data, regexps):\n",
    "            if not regexps:\n",
    "                return list(data)\n",
    "            regexp = self.REGEXPS[regexps[0]]\n",
    "            splitted = regexp.split(data)\n",
    "            tokens = []\n",
    "            for i, split in enumerate(splitted):\n",
    "                if i % 2 == 0:\n",
    "                    tokens += split_by(split, regexps[1:])\n",
    "                else:\n",
    "                    tokens.append(split)\n",
    "            return tokens\n",
    "\n",
    "        tokens = split_by(data, self.REGEXP_ORDER)\n",
    "        if with_begin_and_end:\n",
    "            tokens = [\"^\"] + tokens + [\"$\"]\n",
    "        return tokens\n",
    "\n",
    "    def untokenize(self, tokens : list) -> str:\n",
    "        \"\"\"\n",
    "        Untokenizes a SMILES string.\n",
    "        \"\"\"\n",
    "        smi = \"\"\n",
    "        for token in tokens:\n",
    "            if token == \"$\":\n",
    "                break\n",
    "            if token != \"^\":\n",
    "                smi += token\n",
    "        return smi\n",
    "\n",
    "\n",
    "def create_vocabulary(smiles_list : list, tokenizer : SMILESTokenizer,\n",
    "                      canonical : bool=True) -> Vocabulary:\n",
    "    \"\"\"\n",
    "    Creates a vocabulary for the SMILES syntax.\n",
    "    \"\"\"\n",
    "    if not canonical:\n",
    "        noncanon_smiles_list = []\n",
    "        for smiles in smiles_list:\n",
    "            molecule = rdkit.Chem.MolFromSmiles(smiles)\n",
    "\n",
    "            try:\n",
    "                noncanon_smiles_list.append(\n",
    "                    rdkit.Chem.MolToSmiles(molecule,\n",
    "                                           canonical=False,\n",
    "                                           doRandom=True,\n",
    "                                           isomericSmiles=False)\n",
    "                )\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        smiles_list += noncanon_smiles_list\n",
    "\n",
    "    tokens = set()\n",
    "    for smi in smiles_list:\n",
    "        tokens.update(tokenizer.tokenize(smi, with_begin_and_end=False))\n",
    "\n",
    "    vocabulary = Vocabulary()\n",
    "    vocabulary.update([\"$\", \"^\"] + sorted(tokens))  # end token is 0 (also counts as padding)\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 142 unique tokens in the vocabulary.\n",
      "\n",
      "The unique tokens are: \n",
      "['c', 'C', '(', ')', 'O', '1', '2', '=', 'N', '3', 'n', '[C@H]', '[C@@H]', 'Cl', 'F', '.', '4', 'S', '/', '-', '[O-]', '[C@]', '[nH]', '[C@@]', '[N+]', 'o', 's', '\\\\', '#', '5', '[Na+]', 'P', 'Br', '[n+]', '[Cl-]', '[2H]', '[Br-]', 'I', '[K+]', '[N-]', '[S+]', '[Ca+2]', '6', '[Mg+2]', '[I-]', '[Si]', 'B', '[18F]', '[OH-]', '[n-]', '[Al+3]', '[Al]', '[S@@+]', '[As]', '[123I]', '[Li+]', '[S-]', '[11CH3]', '[Zn+2]', '[Se]', '[Cl+]', '7', '[131I]', '[Mg]', '[I+]', '[K]', '[NH-]', '[PH]', '[B-]', '[125I]', '[N@+]', '[O+]', '[PH2]', '[Sr+2]', '[11C]', '[C-]', '[Ag+]', '[se]', '[P@]', '[Ba+2]', '[Cl+3]', '[Ca]', '[S-2]', '[85Sr+2]', '[O]', '[P+]', '[S@]', '[Te]', '[s+]', '[223Ra]', '[Ra]', '[Ag]', '[TeH]', '[3H]', '[22Na+]', '[Ba]', '[Kr]', '[Mg+]', '[18FH]', '[11C-]', '[AsH3]', '[81Kr]', '[75Se]', '[SrH2]', '[42K+]', '[124I]', '[Rb]', '[85SrH2]', '[LiH]', '[82Rb+]', '[129Xe]', '[CaH2]', '[H+]', '[C]', '[N]', '[F-]', '[Rb+]', '[Cs]', '[131Cs]', '[S@+]', '[13NH3]', '[82Rb]', '[32P]', '[P@@]', '[S]', '[18F-]', '[MgH2]', '[He]', '[O-2]', '[131I-]', '[123I-]', '[13C]', '[15OH2]', '[125I-]', '[14C]', '[89Sr+2]', '[124I-]', '[Xe]', '[45Ca+2]', '[47Ca+2]', '[127Xe]', '[133Xe]']\n"
     ]
    }
   ],
   "source": [
    "# Ihre vorhandenen Klassendefinitionen wie `Vocabulary` und `SMILESTokenizer` bleiben unverändert.\n",
    "\n",
    "# Funktion, um das Vokabular mit Daten aus einer Textdatei zu erstellen\n",
    "def create_vocabulary_from_file(file_path: str, tokenizer: SMILESTokenizer) -> Vocabulary:\n",
    "    vocabulary = Vocabulary()\n",
    "    with open(file_path, 'r') as file:\n",
    "        smiles_data = file.readlines()\n",
    "\n",
    "    # Entfernen von Zeilenumbrüchen und Überprüfen auf leere Zeilen\n",
    "    smiles_data = [line.strip() for line in smiles_data if line.strip()]\n",
    "\n",
    "    # Tokenisieren der SMILES-Daten und Hinzufügen zum Vokabular\n",
    "    for smi in smiles_data:\n",
    "        tokens = tokenizer.tokenize(smi, with_begin_and_end=False)\n",
    "        vocabulary.update(tokens)\n",
    "\n",
    "    return vocabulary\n",
    "\n",
    "# Erstellen einer Instanz von SMILESTokenizer\n",
    "tokenizer = SMILESTokenizer()\n",
    "\n",
    "# Pfad zur SMILES-Textdatei\n",
    "file_path = 'C:\\\\Users\\\\SchockWav3\\\\Desktop\\\\Masterarbeit\\\\chembl_smiles.txt'\n",
    "\n",
    "# Erstellen des Vokabulars aus der Textdatei\n",
    "vocabulary = create_vocabulary_from_file(file_path, tokenizer)\n",
    "\n",
    "# Ausgabe der Informationen über das Vokabular\n",
    "print(f'There are {len(vocabulary)} unique tokens in the vocabulary.\\n')\n",
    "print(f'The unique tokens are: \\n{vocabulary.tokens()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique tokens: 142\n",
      "Tokens and their frequencies:\n",
      "C: 82635\n",
      "(: 42117\n",
      "): 42117\n",
      "=: 17437\n",
      "O: 29965\n",
      "#: 498\n",
      "N: 14270\n",
      "c: 85593\n",
      "1: 27224\n",
      "2: 18214\n",
      "Cl: 3088\n",
      "n: 7103\n",
      ".: 2649\n",
      "S: 2131\n",
      "[C@]: 1026\n",
      "[C@H]: 4492\n",
      "[C@@H]: 4258\n",
      "3: 7312\n",
      "4: 2308\n",
      "[C@@]: 783\n",
      "[nH]: 978\n",
      "-: 1338\n",
      "s: 552\n",
      "[N+]: 753\n",
      "[O-]: 1158\n",
      "/: 1898\n",
      "\\: 530\n",
      "5: 448\n",
      "[Cl-]: 107\n",
      "P: 404\n",
      "o: 585\n",
      "F: 2706\n",
      "[n+]: 160\n",
      "I: 78\n",
      "[Na+]: 419\n",
      "[2H]: 98\n",
      "[N-]: 66\n",
      "Br: 246\n",
      "[Mg+2]: 38\n",
      "[18F]: 26\n",
      "6: 44\n",
      "[As]: 11\n",
      "[S+]: 60\n",
      "[22Na+]: 1\n",
      "[Ca+2]: 55\n",
      "[PH]: 4\n",
      "[S@@+]: 12\n",
      "[Ba]: 1\n",
      "[I+]: 6\n",
      "[Al+3]: 14\n",
      "[I-]: 33\n",
      "[Br-]: 88\n",
      "[K+]: 78\n",
      "[Al]: 14\n",
      "[Si]: 28\n",
      "[Kr]: 1\n",
      "[N@+]: 3\n",
      "[123I]: 11\n",
      "[Mg+]: 1\n",
      "[18FH]: 1\n",
      "[S-]: 8\n",
      "[11C-]: 1\n",
      "[O+]: 3\n",
      "[131I]: 7\n",
      "[AsH3]: 1\n",
      "[K]: 5\n",
      "[PH2]: 3\n",
      "[P@]: 2\n",
      "[Sr+2]: 3\n",
      "[NH-]: 5\n",
      "[Ba+2]: 2\n",
      "[OH-]: 19\n",
      "B: 27\n",
      "[11CH3]: 8\n",
      "[Cl+3]: 2\n",
      "[n-]: 16\n",
      "[Ca]: 2\n",
      "[Zn+2]: 8\n",
      "[S-2]: 2\n",
      "[Li+]: 10\n",
      "[81Kr]: 1\n",
      "[75Se]: 1\n",
      "[Se]: 8\n",
      "[SrH2]: 1\n",
      "[11C]: 3\n",
      "[42K+]: 1\n",
      "[C-]: 3\n",
      "[85Sr+2]: 2\n",
      "[O]: 2\n",
      "[P+]: 2\n",
      "[124I]: 1\n",
      "[Rb]: 1\n",
      "[85SrH2]: 1\n",
      "[Ag+]: 3\n",
      "[S@]: 2\n",
      "[Te]: 2\n",
      "[s+]: 2\n",
      "[LiH]: 1\n",
      "[82Rb+]: 1\n",
      "[129Xe]: 1\n",
      "[CaH2]: 1\n",
      "[B-]: 4\n",
      "[223Ra]: 2\n",
      "[H+]: 1\n",
      "[C]: 1\n",
      "[Cl+]: 8\n",
      "[N]: 1\n",
      "[F-]: 1\n",
      "[Rb+]: 1\n",
      "[Cs]: 1\n",
      "[Ra]: 2\n",
      "[131Cs]: 1\n",
      "[S@+]: 1\n",
      "[13NH3]: 1\n",
      "[82Rb]: 1\n",
      "[32P]: 1\n",
      "[P@@]: 1\n",
      "[Ag]: 2\n",
      "[se]: 3\n",
      "[S]: 1\n",
      "[18F-]: 1\n",
      "[MgH2]: 1\n",
      "[Mg]: 7\n",
      "[He]: 1\n",
      "[125I]: 4\n",
      "[TeH]: 2\n",
      "7: 8\n",
      "[O-2]: 1\n",
      "[3H]: 2\n",
      "[131I-]: 1\n",
      "[123I-]: 1\n",
      "[13C]: 1\n",
      "[15OH2]: 1\n",
      "[125I-]: 1\n",
      "[14C]: 1\n",
      "[89Sr+2]: 1\n",
      "[124I-]: 1\n",
      "[Xe]: 1\n",
      "[45Ca+2]: 1\n",
      "[47Ca+2]: 1\n",
      "[127Xe]: 1\n",
      "[133Xe]: 1\n"
     ]
    }
   ],
   "source": [
    "# Nachdem das Vokabular erstellt wurde\n",
    "print(f\"Total unique tokens: {len(vocabulary)}\")\n",
    "print(\"Tokens and their frequencies:\")\n",
    "for token, frequency in vocabulary.frequencies().items():\n",
    "    print(f\"{token}: {frequency}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "deepSMILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSMILES [0]: CCC)CC=O)OCC#N))cccccOcccccc6)))))))c6)))))))))ccccCl)cc6\n",
      "Token List [0]: ['C', 'C', 'C', ')', 'C', 'C', '=', 'O', ')', 'O', 'C', 'C', '#', 'N', ')', ')', 'c', 'c', 'c', 'c', 'c', 'O', 'c', 'c', 'c', 'c', 'c', 'c', '6', ')', ')', ')', ')', ')', ')', ')', 'c', '6', ')', ')', ')', ')', ')', ')', ')', ')', ')', 'c', 'c', 'c', 'c', 'Cl', ')', 'c', 'c', '6']\n",
      "DeepSMILES [1000]: FccccCCCCNCCNCCNcccccc6)))))))))CC6)))))))))ccccF)cc6)))))))cc6\n",
      "Token List [1000]: ['F', 'c', 'c', 'c', 'c', 'C', 'C', 'C', 'C', 'N', 'C', 'C', 'N', 'C', 'C', 'N', 'c', 'c', 'c', 'c', 'c', 'c', '6', ')', ')', ')', ')', ')', ')', ')', ')', ')', 'C', 'C', '6', ')', ')', ')', ')', ')', ')', ')', ')', ')', 'c', 'c', 'c', 'c', 'F', ')', 'c', 'c', '6', ')', ')', ')', ')', ')', ')', ')', 'c', 'c', '6']\n",
      "DeepSMILES [2000]: CccccCO)CC)NCCCcccccc6))))))))))))cc6O\n",
      "Token List [2000]: ['C', 'c', 'c', 'c', 'c', 'C', 'O', ')', 'C', 'C', ')', 'N', 'C', 'C', 'C', 'c', 'c', 'c', 'c', 'c', 'c', '6', ')', ')', ')', ')', ')', ')', ')', ')', ')', ')', ')', ')', 'c', 'c', '6', 'O']\n",
      "DeepSMILES [3000]: O=c[nH]c=S)[n-]cc6I.[Na+]\n",
      "Token List [3000]: ['O', '=', 'c', '[nH]', 'c', '=', 'S', ')', '[n-]', 'c', 'c', '6', 'I', '.', '[Na+]']\n",
      "DeepSMILES [4000]: CC/C=C\\C)CCC))C=O)NC=O)NC6=O\n",
      "Token List [4000]: ['C', 'C', '/', 'C', '=', 'C', '\\\\', 'C', ')', 'C', 'C', 'C', ')', ')', 'C', '=', 'O', ')', 'N', 'C', '=', 'O', ')', 'N', 'C', '6', '=', 'O']\n",
      "DeepSMILES [5000]: CC=O)[C@H]CC[C@H][C@@H]C[C@@H]O)[C@]C[C@H]3CC[C@]6C)[C@H]%10CC[C@]%17%14C\n",
      "Token List [5000]: ['C', 'C', '=', 'O', ')', '[C@H]', 'C', 'C', '[C@H]', '[C@@H]', 'C', '[C@@H]', 'O', ')', '[C@]', 'C', '[C@H]', '3', 'C', 'C', '[C@]', '6', 'C', ')', '[C@H]', '%10', 'C', 'C', '[C@]', '%17', '%14', 'C']\n",
      "DeepSMILES [6000]: C[C@@H]OccccOccnccccCl)cc6n%10)))))))))))cc6)))))))C=O)O\n",
      "Token List [6000]: ['C', '[C@@H]', 'O', 'c', 'c', 'c', 'c', 'O', 'c', 'c', 'n', 'c', 'c', 'c', 'c', 'Cl', ')', 'c', 'c', '6', 'n', '%10', ')', ')', ')', ')', ')', ')', ')', ')', ')', ')', ')', 'c', 'c', '6', ')', ')', ')', ')', ')', ')', ')', 'C', '=', 'O', ')', 'O']\n",
      "DeepSMILES [7000]: O=cocccccc6cO)c%10CCCO)ccccCl)cc6))))))))ccccCl)s5\n",
      "Token List [7000]: ['O', '=', 'c', 'o', 'c', 'c', 'c', 'c', 'c', 'c', '6', 'c', 'O', ')', 'c', '%10', 'C', 'C', 'C', 'O', ')', 'c', 'c', 'c', 'c', 'Cl', ')', 'c', 'c', '6', ')', ')', ')', ')', ')', ')', ')', ')', 'c', 'c', 'c', 'c', 'Cl', ')', 's', '5']\n",
      "DeepSMILES [8000]: CNCCCCCCC8)C6OC=O)CO)cccccc6))))))CCCCC5.Cl\n",
      "Token List [8000]: ['C', 'N', 'C', 'C', 'C', 'C', 'C', 'C', 'C', '8', ')', 'C', '6', 'O', 'C', '=', 'O', ')', 'C', 'O', ')', 'c', 'c', 'c', 'c', 'c', 'c', '6', ')', ')', ')', ')', ')', ')', 'C', 'C', 'C', 'C', 'C', '5', '.', 'Cl']\n",
      "DeepSMILES [9000]: CNC)cncNC)C))ncNC)C))n6\n",
      "Token List [9000]: ['C', 'N', 'C', ')', 'c', 'n', 'c', 'N', 'C', ')', 'C', ')', ')', 'n', 'c', 'N', 'C', ')', 'C', ')', ')', 'n', '6']\n",
      "DeepSMILES [10000]: CNCCNCCCNcccccc6SccccCF)F)F))cc6%14)))))))))))))))))CC6\n",
      "Token List [10000]: ['C', 'N', 'C', 'C', 'N', 'C', 'C', 'C', 'N', 'c', 'c', 'c', 'c', 'c', 'c', '6', 'S', 'c', 'c', 'c', 'c', 'C', 'F', ')', 'F', ')', 'F', ')', ')', 'c', 'c', '6', '%14', ')', ')', ')', ')', ')', ')', ')', ')', ')', ')', ')', ')', ')', ')', ')', ')', ')', 'C', 'C', '6']\n",
      "There are 157 unique tokens in the vocabulary.\n",
      "\n",
      "The unique tokens are: \n",
      "[')', 'c', 'C', 'O', '=', '6', 'N', 'n', '[C@H]', '[C@@H]', '5', 'Cl', 'F', '.', 'S', '%10', '/', '9', '-', '[O-]', '[C@]', '[nH]', '[C@@]', '[N+]', '%14', 'o', 's', '\\\\', '#', '3', '[Na+]', 'P', '7', '%17', '%11', '8', '4', 'Br', '%13', '%15', '[n+]', '[Cl-]', '%12', '[2H]', '[Br-]', 'I', '[K+]', '[N-]', '[S+]', '[Ca+2]', '%18', '[Mg+2]', '%20', '[I-]', '[Si]', 'B', '%16', '[18F]', '%19', '[OH-]', '[n-]', '[Al+3]', '[Al]', '%21', '[S@@+]', '[As]', '[123I]', '[Li+]', '[S-]', '[11CH3]', '[Zn+2]', '[Se]', '[Cl+]', '[131I]', '[Mg]', '[I+]', '[K]', '[NH-]', '[PH]', '[B-]', '[125I]', '[N@+]', '[O+]', '[PH2]', '[Sr+2]', '%24', '[11C]', '[C-]', '[Ag+]', '[se]', '%25', '[P@]', '[Ba+2]', '[Cl+3]', '[Ca]', '[S-2]', '[85Sr+2]', '[O]', '[P+]', '%22', '[S@]', '[Te]', '[s+]', '[223Ra]', '[Ra]', '[Ag]', '[TeH]', '[3H]', '[22Na+]', '[Ba]', '[Kr]', '[Mg+]', '[18FH]', '[11C-]', '[AsH3]', '[81Kr]', '[75Se]', '[SrH2]', '[42K+]', '[124I]', '[Rb]', '[85SrH2]', '[LiH]', '[82Rb+]', '[129Xe]', '%23', '[CaH2]', '[H+]', '[C]', '[N]', '[F-]', '[Rb+]', '[Cs]', '[131Cs]', '[S@+]', '[13NH3]', '[82Rb]', '[32P]', '[P@@]', '[S]', '[18F-]', '[MgH2]', '[He]', '[O-2]', '[131I-]', '[123I-]', '[13C]', '[15OH2]', '[125I-]', '[14C]', '[89Sr+2]', '[124I-]', '[Xe]', '[45Ca+2]', '[47Ca+2]', '[127Xe]', '[133Xe]']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from typing import Union, Dict\n",
    "import rdkit\n",
    "import deepsmiles\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self, tokens: Union[Dict[str, int], None] = None, starting_id: int = 0) -> None:\n",
    "        self._tokens = {}\n",
    "        self._frequencies = {}\n",
    "        self._current_id = starting_id\n",
    "\n",
    "        if tokens:\n",
    "            for token, idx in tokens.items():\n",
    "                self._add(token, idx)\n",
    "                self._current_id = max(self._current_id, idx + 1)\n",
    "\n",
    "    def __getitem__(self, token_or_id: Union[str, int]) -> int:\n",
    "        return self._tokens[token_or_id]\n",
    "\n",
    "    def add(self, token: str) -> int:\n",
    "        if not isinstance(token, str):\n",
    "            raise TypeError(\"Token is not a string\")\n",
    "        if token in self._tokens:\n",
    "            self._frequencies[token] += 1\n",
    "            return self._tokens[token]\n",
    "        else:\n",
    "            self._add(token, self._current_id)\n",
    "            self._frequencies[token] = 1\n",
    "            self._current_id += 1\n",
    "            return self._current_id - 1\n",
    "\n",
    "    def update(self, tokens: list) -> None:\n",
    "        for token in tokens:\n",
    "            self.add(token)\n",
    "\n",
    "    def __delitem__(self, token_or_id: Union[str, int]) -> None:\n",
    "        other_val = self._tokens[token_or_id]\n",
    "        del self._tokens[other_val]\n",
    "        del self._tokens[token_or_id]\n",
    "        del self._frequencies[other_val if isinstance(token_or_id, int) else token_or_id]\n",
    "\n",
    "    def __contains__(self, token_or_id: Union[str, int]) -> bool:\n",
    "        return token_or_id in self._tokens\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self._tokens) // 2\n",
    "\n",
    "    def tokens(self) -> list:\n",
    "        return sorted(self._frequencies, key=self._frequencies.get, reverse=True)\n",
    "\n",
    "    def frequencies(self) -> Dict[str, int]:\n",
    "        return self._frequencies\n",
    "\n",
    "    def _add(self, token: str, idx: int) -> None:\n",
    "        if idx not in self._tokens:\n",
    "            self._tokens[token] = idx\n",
    "            self._tokens[idx] = token\n",
    "            self._frequencies[token] = 0\n",
    "        else:\n",
    "            raise ValueError(\"IDX already present in vocabulary\")\n",
    "\n",
    "\n",
    "class DeepSMILESTokenizer:\n",
    "    def __init__(self):\n",
    "        self.converter = deepsmiles.Converter(rings=True, branches=True)\n",
    "\n",
    "        self.REGEXPS = {\n",
    "            \"brackets\": re.compile(r\"(\\[[^\\]]*\\])\"),\n",
    "            \"2_ring_nums\": re.compile(r\"(%\\d{2})\"),\n",
    "            \"brcl\": re.compile(r\"(Br|Cl)\")\n",
    "        }\n",
    "        self.REGEXP_ORDER = [\"brackets\", \"2_ring_nums\", \"brcl\"]\n",
    "\n",
    "    def encode(self, smiles: str) -> str:\n",
    "        try:\n",
    "            deep_smiles = self.converter.encode(smiles)\n",
    "            return deep_smiles\n",
    "        except Exception as e:\n",
    "            print(\"Could not convert SMILES to DeepSMILES:\", e)\n",
    "            return \"UNK\"\n",
    "\n",
    "    def tokenize(self, deep_smiles: str, with_begin_and_end: bool = True) -> list:\n",
    "        def split_by(data, regexps):\n",
    "            if not regexps:\n",
    "                return list(data)\n",
    "            regexp = self.REGEXPS[regexps[0]]\n",
    "            splitted = regexp.split(data)\n",
    "            tokens = []\n",
    "            for i, split in enumerate(splitted):\n",
    "                if i % 2 == 0:\n",
    "                    tokens += split_by(split, regexps[1:])\n",
    "                else:\n",
    "                    tokens.append(split)\n",
    "            return tokens\n",
    "\n",
    "        tokens = split_by(deep_smiles, self.REGEXP_ORDER)\n",
    "        if with_begin_and_end:\n",
    "            tokens = [\"^\"] + tokens + [\"$\"]\n",
    "        return tokens\n",
    "\n",
    "    def untokenize(self, tokens: list) -> str:\n",
    "        return ''.join(token for token in tokens if token not in ['^', '$', 'UNK'])\n",
    "\n",
    "\n",
    "def create_vocabulary_from_file(file_path: str, tokenizer: DeepSMILESTokenizer) -> Vocabulary:\n",
    "    \n",
    "    vocabulary = Vocabulary()\n",
    "    \n",
    "    with open(file_path, 'r') as file:\n",
    "        smiles_data = file.readlines()\n",
    "\n",
    "    smiles_data = [line.strip() for line in smiles_data if line.strip()]\n",
    "\n",
    "    for i, smi in enumerate(smiles_data):\n",
    "        deep_smiles = tokenizer.encode(smi)\n",
    "        if i % 1000 == 0:\n",
    "            print(f\"DeepSMILES [{i}]: {deep_smiles}\")  # Debugging-Ausgabe\n",
    "        token_list = tokenizer.tokenize(deep_smiles, with_begin_and_end=False)\n",
    "        if i % 1000 == 0:\n",
    "            print(f\"Token List [{i}]: {token_list}\")  # Debugging-Ausgabe\n",
    "        vocabulary.update(token_list)\n",
    "    \n",
    "    return vocabulary\n",
    "\n",
    "# Pfad zur SMILES-Textdatei\n",
    "file_path = 'C:\\\\Users\\\\SchockWav3\\\\Desktop\\\\Masterarbeit\\\\chembl_smiles.txt'\n",
    "\n",
    "# Erstellen einer Instanz von DeepSMILESTokenizer\n",
    "tokenizer = DeepSMILESTokenizer()\n",
    "\n",
    "# Erstellen des Vokabulars aus der Textdatei\n",
    "vocabulary = create_vocabulary_from_file(file_path, tokenizer)\n",
    "\n",
    "# Ausgabe der Informationen über das Vokabular\n",
    "print(f'There are {len(vocabulary)} unique tokens in the vocabulary.\\n')\n",
    "print(f'The unique tokens are: \\n{vocabulary.tokens()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique tokens: 157\n",
      "Tokens and their frequencies:\n",
      "C: 82635\n",
      "): 113298\n",
      "=: 17437\n",
      "O: 29965\n",
      "#: 498\n",
      "N: 14270\n",
      "c: 85593\n",
      "6: 16786\n",
      "Cl: 3088\n",
      "n: 7103\n",
      "5: 4252\n",
      ".: 2649\n",
      "S: 2131\n",
      "[C@]: 1026\n",
      "[C@H]: 4492\n",
      "[C@@H]: 4258\n",
      "%10: 1950\n",
      "[C@@]: 783\n",
      "%14: 614\n",
      "%17: 334\n",
      "[nH]: 978\n",
      "9: 1622\n",
      "-: 1338\n",
      "s: 552\n",
      "[N+]: 753\n",
      "[O-]: 1158\n",
      "%16: 26\n",
      "%12: 106\n",
      "/: 1898\n",
      "\\: 530\n",
      "%13: 235\n",
      "%20: 33\n",
      "[Cl-]: 107\n",
      "P: 404\n",
      "o: 585\n",
      "F: 2706\n",
      "[n+]: 160\n",
      "8: 253\n",
      "I: 78\n",
      "[Na+]: 419\n",
      "3: 426\n",
      "[2H]: 98\n",
      "%15: 200\n",
      "%11: 264\n",
      "[N-]: 66\n",
      "7: 345\n",
      "Br: 246\n",
      "[Mg+2]: 38\n",
      "[18F]: 26\n",
      "%18: 43\n",
      "[As]: 11\n",
      "[S+]: 60\n",
      "[22Na+]: 1\n",
      "4: 249\n",
      "[Ca+2]: 55\n",
      "%25: 2\n",
      "[PH]: 4\n",
      "[S@@+]: 12\n",
      "[Ba]: 1\n",
      "[I+]: 6\n",
      "[Al+3]: 14\n",
      "[I-]: 33\n",
      "[Br-]: 88\n",
      "[K+]: 78\n",
      "[Al]: 14\n",
      "[Si]: 28\n",
      "%21: 13\n",
      "%19: 20\n",
      "[Kr]: 1\n",
      "[N@+]: 3\n",
      "[123I]: 11\n",
      "[Mg+]: 1\n",
      "[18FH]: 1\n",
      "[S-]: 8\n",
      "[11C-]: 1\n",
      "[O+]: 3\n",
      "[131I]: 7\n",
      "[AsH3]: 1\n",
      "[K]: 5\n",
      "[PH2]: 3\n",
      "[P@]: 2\n",
      "[Sr+2]: 3\n",
      "[NH-]: 5\n",
      "[Ba+2]: 2\n",
      "[OH-]: 19\n",
      "B: 27\n",
      "[11CH3]: 8\n",
      "[Cl+3]: 2\n",
      "[n-]: 16\n",
      "[Ca]: 2\n",
      "[Zn+2]: 8\n",
      "%24: 3\n",
      "[S-2]: 2\n",
      "[Li+]: 10\n",
      "[81Kr]: 1\n",
      "[75Se]: 1\n",
      "[Se]: 8\n",
      "[SrH2]: 1\n",
      "[11C]: 3\n",
      "[42K+]: 1\n",
      "[C-]: 3\n",
      "[85Sr+2]: 2\n",
      "[O]: 2\n",
      "[P+]: 2\n",
      "[124I]: 1\n",
      "[Rb]: 1\n",
      "[85SrH2]: 1\n",
      "[Ag+]: 3\n",
      "%22: 2\n",
      "[S@]: 2\n",
      "[Te]: 2\n",
      "[s+]: 2\n",
      "[LiH]: 1\n",
      "[82Rb+]: 1\n",
      "[129Xe]: 1\n",
      "%23: 1\n",
      "[CaH2]: 1\n",
      "[B-]: 4\n",
      "[223Ra]: 2\n",
      "[H+]: 1\n",
      "[C]: 1\n",
      "[Cl+]: 8\n",
      "[N]: 1\n",
      "[F-]: 1\n",
      "[Rb+]: 1\n",
      "[Cs]: 1\n",
      "[Ra]: 2\n",
      "[131Cs]: 1\n",
      "[S@+]: 1\n",
      "[13NH3]: 1\n",
      "[82Rb]: 1\n",
      "[32P]: 1\n",
      "[P@@]: 1\n",
      "[Ag]: 2\n",
      "[se]: 3\n",
      "[S]: 1\n",
      "[18F-]: 1\n",
      "[MgH2]: 1\n",
      "[Mg]: 7\n",
      "[He]: 1\n",
      "[125I]: 4\n",
      "[TeH]: 2\n",
      "[O-2]: 1\n",
      "[3H]: 2\n",
      "[131I-]: 1\n",
      "[123I-]: 1\n",
      "[13C]: 1\n",
      "[15OH2]: 1\n",
      "[125I-]: 1\n",
      "[14C]: 1\n",
      "[89Sr+2]: 1\n",
      "[124I-]: 1\n",
      "[Xe]: 1\n",
      "[45Ca+2]: 1\n",
      "[47Ca+2]: 1\n",
      "[127Xe]: 1\n",
      "[133Xe]: 1\n"
     ]
    }
   ],
   "source": [
    "# Nachdem das Vokabular erstellt wurde\n",
    "print(f\"Total unique tokens: {len(vocabulary)}\")\n",
    "print(\"Tokens and their frequencies:\")\n",
    "for token, frequency in vocabulary.frequencies().items():\n",
    "    print(f\"{token}: {frequency}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
